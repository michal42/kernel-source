From: jbeulich@suse.com
Subject: privcmd: allow preempting long running user-mode originating hypercalls
Patch-mainline: n/a
References: bnc#861093

--- sle12.orig/arch/x86/include/mach-xen/asm/hypervisor.h	2012-05-11 16:22:53.000000000 +0200
+++ sle12/arch/x86/include/mach-xen/asm/hypervisor.h	2014-01-22 15:07:39.000000000 +0100
@@ -223,6 +223,9 @@ static inline int gnttab_post_map_adjust
 #ifdef CONFIG_XEN
 #define is_running_on_xen() 1
 extern char hypercall_page[PAGE_SIZE];
+#define in_hypercall(regs) (!user_mode_vm(regs) && \
+	(regs)->ip >= (unsigned long)hypercall_page && \
+	(regs)->ip < (unsigned long)hypercall_page + PAGE_SIZE)
 #else
 extern char *hypercall_stubs;
 #define is_running_on_xen() (!!hypercall_stubs)
--- sle12.orig/arch/x86/kernel/entry_64-xen.S	2014-09-01 16:27:47.000000000 +0200
+++ sle12/arch/x86/kernel/entry_64-xen.S	2014-01-16 10:52:27.000000000 +0100
@@ -1129,6 +1129,20 @@ ENTRY(do_hypervisor_callback)   # do_hyp
 	popq %rsp
 	CFI_DEF_CFA_REGISTER rsp
 	decl PER_CPU_VAR(irq_count)
+#ifndef CONFIG_PREEMPT
+	test %al,%al
+	jz   error_exit
+	GET_THREAD_INFO(%rdx)
+	cmpl $0,TI_preempt_count(%rdx)
+	jnz  error_exit
+	bt   $TIF_NEED_RESCHED,TI_flags(%rdx)
+	jnc  error_exit
+	bt   $9,EFLAGS-ARGOFFSET(%rsp)
+	jnc  error_exit
+	movb $0,PER_CPU_VAR(privcmd_hcall)
+	call preempt_schedule_irq
+	movb $1,PER_CPU_VAR(privcmd_hcall)
+#endif
 	jmp  error_exit
 	CFI_ENDPROC
 END(do_hypervisor_callback)
--- sle12.orig/drivers/xen/core/evtchn.c	2013-08-08 11:52:54.000000000 +0200
+++ sle12/drivers/xen/core/evtchn.c	2014-03-31 14:37:37.000000000 +0200
@@ -373,7 +373,14 @@ static DEFINE_PER_CPU(unsigned int, curr
 static DEFINE_PER_CPU(unsigned int, current_l2i);
 
 /* NB. Interrupts are disabled on entry. */
-asmlinkage void __irq_entry evtchn_do_upcall(struct pt_regs *regs)
+asmlinkage
+#ifdef CONFIG_PREEMPT
+void
+#define return(x) return
+#else
+bool
+#endif
+__irq_entry evtchn_do_upcall(struct pt_regs *regs)
 {
 	unsigned long       l1, l2;
 	unsigned long       masked_l1, masked_l2;
@@ -388,7 +395,7 @@ asmlinkage void __irq_entry evtchn_do_up
 		__this_cpu_or(upcall_state, UPC_NESTED_LATCH);
 		/* Avoid a callback storm when we reenable delivery. */
 		vcpu_info->evtchn_upcall_pending = 0;
-		return;
+		return(false);
 	}
 
 	old_regs = set_irq_regs(regs);
@@ -504,6 +511,9 @@ asmlinkage void __irq_entry evtchn_do_up
 	irq_exit();
 	xen_spin_irq_exit();
 	set_irq_regs(old_regs);
+
+	return(__this_cpu_read(privcmd_hcall) && in_hypercall(regs));
+#undef return
 }
 
 /*
--- sle12.orig/drivers/xen/privcmd/privcmd.c	2014-07-02 12:21:15.000000000 +0200
+++ sle12/drivers/xen/privcmd/privcmd.c	2014-07-02 12:23:13.000000000 +0200
@@ -24,6 +24,18 @@
 #include <xen/interface/xen.h>
 #include <xen/xen_proc.h>
 #include <xen/features.h>
+#include <xen/evtchn.h>
+
+#ifndef CONFIG_PREEMPT
+DEFINE_PER_CPU(bool, privcmd_hcall);
+#endif
+
+static inline void _privcmd_hcall(bool state)
+{
+#ifndef CONFIG_PREEMPT
+	this_cpu_write(privcmd_hcall, state);
+#endif
+}
 
 #ifndef HAVE_ARCH_PRIVCMD_MMAP
 static int enforce_singleshot_mapping_fn(pte_t *pte, struct page *pmd_page,
@@ -67,6 +79,7 @@ static long privcmd_ioctl(struct file *f
 		ret = -ENOSYS;
 		if (hypercall.op >= (PAGE_SIZE >> 5))
 			break;
+		_privcmd_hcall(true);
 		stac();
 		ret = _hypercall(long, (unsigned int)hypercall.op,
 				 (unsigned long)hypercall.arg[0],
@@ -76,8 +89,10 @@ static long privcmd_ioctl(struct file *f
 				 (unsigned long)hypercall.arg[4]);
 		clac();
 #else
+		_privcmd_hcall(true);
 		ret = privcmd_hypercall(&hypercall);
 #endif
+		_privcmd_hcall(false);
 	}
 	break;
 
--- sle12.orig/include/xen/evtchn.h	2013-08-08 11:52:01.000000000 +0200
+++ sle12/include/xen/evtchn.h	2014-01-15 14:32:14.000000000 +0100
@@ -144,7 +144,13 @@ void irq_resume(void);
 #endif
 
 /* Entry point for notifications into Linux subsystems. */
-asmlinkage void evtchn_do_upcall(struct pt_regs *regs);
+asmlinkage
+#ifdef CONFIG_PREEMPT
+void
+#else
+bool
+#endif
+evtchn_do_upcall(struct pt_regs *regs);
 
 /* Mark a PIRQ as unavailable for dynamic allocation. */
 void evtchn_register_pirq(int irq);
@@ -209,6 +215,8 @@ void notify_remote_via_ipi(unsigned int 
 void clear_ipi_evtchn(void);
 #endif
 
+DECLARE_PER_CPU(bool, privcmd_hcall);
+
 #if defined(CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING) \
     && CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING
 void xen_spin_irq_enter(void);
--- sle12.orig/kernel/sched/core.c	2014-09-01 15:44:03.000000000 +0200
+++ sle12/kernel/sched/core.c	2014-02-06 16:11:50.000000000 +0100
@@ -2717,6 +2717,9 @@ asmlinkage void __sched notrace preempt_
 }
 EXPORT_SYMBOL(preempt_schedule);
 
+#endif
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_XEN)
+
 /*
  * this is the entry point to schedule() from kernel preemption
  * off of irq context.
