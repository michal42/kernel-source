From: Mel Gorman <mgorman@suse.de>
Date: Tue, 10 Dec 2013 07:02:24 +0000
Subject: x86: mm: Account for TLB flushes only when debugging

References: VM Performance
Patch-mainline: v3.14

Bisection between 3.11 and 3.12 fingered commit 9824cf97 (mm: vmstats:
tlb flush counters).  The counters are undeniably useful but how often
do we really need to debug TLB flush related issues? It does not justify
taking the penalty everywhere so make it a debugging option.

Signed-off-by: Mel Gorman <mgorman@suse.de>
Tested-by: Davidlohr Bueso <davidlohr@hp.com>
Reviewed-by: Rik van Riel <riel@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Cc: Hugh Dickins <hughd@google.com>
Cc: Alex Shi <alex.shi@linaro.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Peter Zijlstra <peterz@infradead.org>
Link: http://lkml.kernel.org/n/tip-XzxjntugxuwpxXhcrxqqh53b@git.kernel.org
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Automatically created from "patches.suse/x86-mm-Account-for-TLB-flushes-only-when-debugging.patch" by xen-port-patches.py

--- sle12.orig/arch/x86/include/mach-xen/asm/tlbflush.h	2013-10-10 17:49:53.000000000 +0200
+++ sle12/arch/x86/include/mach-xen/asm/tlbflush.h	2014-01-09 12:02:13.000000000 +0100
@@ -19,7 +19,7 @@ static inline void __flush_tlb_all(void)
 
 static inline void __flush_tlb_one(unsigned long addr)
 {
-	count_vm_event(NR_TLB_LOCAL_FLUSH_ONE);
+	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 	__flush_tlb_single(addr);
 }
 
@@ -49,13 +49,13 @@ static inline void __flush_tlb_one(unsig
  */
 static inline void __flush_tlb_up(void)
 {
-	count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
+	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 	__flush_tlb();
 }
 
 static inline void flush_tlb_all(void)
 {
-	count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
+	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 	__flush_tlb_all();
 }
 
--- sle12.orig/arch/x86/mm/tlb-xen.c	2013-09-26 15:33:39.000000000 +0200
+++ sle12/arch/x86/mm/tlb-xen.c	2014-01-09 12:02:13.000000000 +0100
@@ -74,12 +74,12 @@ void flush_tlb_mm_range(struct mm_struct
 	    && !has_large_page(mm, start, end)) {
 		/* flush range by one by one 'invlpg' */
 		for (addr = start; addr < end; addr += PAGE_SIZE) {
-			count_vm_event(NR_TLB_LOCAL_FLUSH_ONE);
+			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 			xen_invlpg_mask(mask, addr);
 		}
 	} else {
 flush_all:
-		count_vm_event(NR_TLB_LOCAL_FLUSH_ALL);
+		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 		xen_tlb_flush_mask(mask);
 	}
 
